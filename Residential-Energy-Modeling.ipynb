{"cells":[{"cell_type":"markdown","metadata":{"id":"xNqZDLtHQ-lu"},"source":["<h2><center>Residential Energy Consumption Prediction using Regression and Ensemble Methods!</center></h2>\n","\n","### Table of Contents\n","\n","1. [Introduction](#1)\n","\n","2. [Project Objectives](#2)\n","\n","3. [Loading Libraries and Reading Data](#3)<br>\n","    a. [Loading Libraries](#31)<br>\n","    b. [Version of Installed Python Library](#32)<br>\n","    c. [Loading Data](#33)<br>\n","\n","4. [Exploratory Data Analysis (EDA)](#4)<br>\n","    a. [Data Dimensionality](#41)<br>\n","    b. [Data Types](#42)<br>\n","    c. [Summary Statistics](#43)<br>\n","    d. [Check for Missing Values](#44)<br>\n","    e. [Explore Target Variable and Extract Important Features](#45)<br>\n","    f. [Detect Outliers and Anomalies](#46)<br>\n","    g. [Explore Additional Predictor Variables](#47)<br>\n","    \n","5. [Data Transformation and Preprocessing](#5)<br>\n","    a. [Data Transformation](#51)<br>\n","    &emsp;i. &nbsp;[Combining/Merging Predictor Features](#511)<br>\n","    &emsp;ii. [Combining/Merging Levels with Low Frequency in Discrete Predictor Features](#512)<br>\n","    \n","    b. [Data Preprocessing](#52)<br>\n","    &emsp;i.&ensp; [Removing Predictor Features with high 'Not Applicable' Values](#521)<br>\n","    &emsp;ii.&nbsp; [Removing Imputation Flags](#522)<br>\n","    &emsp;iii. [Removing Duplicate Features](#523)<br>\n","    &emsp;iv.&nbsp;[Removing Outliers](#524)<br>\n","\n","6. [Feature Engineering](#6)<br>\n","    a. [Exploratory Feature Reduction](#61)<br>\n","    b. [Feature Selection](#62)<br>\n","    &emsp;i. &nbsp;[Find Features with Single Unique Value](#621)<br>\n","    &emsp;ii. [Find Collinear Features](#622)<br>\n","    &emsp;iii. [Find Features with Zero Importance using GBM](#623)<br>\n","    &emsp;iv.&nbsp;[Find Features with Low Importance](#624)<br>\n","    &emsp;v.&nbsp;[Removing Features](#625)<br>\n","\n","7. [Model Development & Comparison](#7)<br>\n","    a. [Building Baseline Models with Default Params](#71)<br>\n","    b. [Hyperparameter Tuning & Model Comparison](#72)<br>\n","    c. [Model Evaluation on Unseen Data](#73)"]},{"cell_type":"markdown","metadata":{"id":"oCd_z6F6Q-lz"},"source":["<a id=\"1\"></a>\n","### 1. Introduction\n","\n","Every four years, [EIA](https://www.eia.gov/consumption/residential/about.php) administers the Residential Energy Consumption Survey (RECS) to a nationally representative sample of housing units across the United States to collect energy characteristics data on the housing unit, usage patterns, and household demographics. $^{1}$\n","\n","This project focuses on 2009 RECS survey data which represents the 13th iteration of the RECS program. First conducted in 1978, the Residential Energy Consumption Survey is a national sample survey that collects energy-related data for housing units occupied as a primary residence and the households that live in them. 2009 data were collected from 12,083 households selected at random using a complex multistage, area-probability sample design. The sample represents 113.6 million U.S. households, the Census Bureauâ€™s statistical estimate for all occupied housing units in 2009 derived from their American Community Survey (ACS). $^{1}$"]},{"cell_type":"markdown","metadata":{"id":"zCtyokRAQ-l0"},"source":["<a id=\"2\"></a>\n","### 2. Project objectives\n","\n","This project utilizes applied machine learning methods, Regression as well as Ensemble methods, on the 2009 RECS dataset to achieve the following three objectives:\n","\n","- __Description:__ Describing/Exploring the set of features with the strong statistical associations with target variable Annual Electricity Usage (in kWh)\n","\n","- __Feature Selection:__ Showcasing different feature selection methods to select the most critical features for electricity consumption;\n","\n","- __Prediction:__ â€“ Applying different machine learning models to measure and compare predictive performance of the selected features\n","\n","\n","The target variable is \"KWH\" which stands for kilowatt-hour."]},{"cell_type":"markdown","metadata":{"id":"lUbGCxUiQ-l1"},"source":["<a id=\"3\"></a>\n","### 3. Loading Libraries and Reading Data\n","<a id=\"31\"></a>\n","#### a. Loading Libraries\n","Let's start by importing the libraries we need"]},{"cell_type":"code","source":["#!pip install --upgrade \"pandas==2.2.2\" \"numpy==2.2.0\" \"scipy==1.13.0\" --force-reinstall --quiet\n","#import os\n","#os.kill(os.getpid(), 9)  # Restart runtime\n"],"metadata":{"id":"eLxAFa1NrhO0","executionInfo":{"status":"ok","timestamp":1754631581598,"user_tz":-330,"elapsed":19,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","# ðŸ©¹ Restore deprecated methods for compatibility\n","if not hasattr(pd.DataFrame, 'append'):\n","    def _append(self, other, ignore_index=False, verify_integrity=False, sort=False):\n","        from pandas import concat, DataFrame\n","        if isinstance(other, dict): other = DataFrame([other])\n","        elif not isinstance(other, DataFrame): other = DataFrame(other)\n","        return concat([self, other], ignore_index=ignore_index, verify_integrity=verify_integrity, sort=sort)\n","    pd.DataFrame.append = _append\n","\n","if not hasattr(pd.Index, '_format_flat'):\n","    pd.Index._format_flat = lambda self, include_name=False: self.tolist()\n","\n"],"metadata":{"id":"nt5h6qDYrmD3","executionInfo":{"status":"ok","timestamp":1754631581724,"user_tz":-330,"elapsed":130,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","execution_count":3,"metadata":{"id":"TV25nSRaQ-l1","executionInfo":{"status":"error","timestamp":1754631596640,"user_tz":-330,"elapsed":14917,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}},"outputId":"a18cb107-dd48-44a7-b391-1f9b6d75d7ce","colab":{"base_uri":"https://localhost:8080/","height":385}},"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'feature_selector'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4078384128.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfeature_selector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureSelector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m#XGBoost libraries\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'feature_selector'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}],"source":["import pandas as pd\n","from pandas import set_option\n","from pandas import get_dummies\n","import numpy as np\n","import os\n","\n","import matplotlib.pyplot as plt\n","import matplotlib\n","%matplotlib inline\n","import seaborn as sns\n","import scipy.stats\n","\n","from sklearn import model_selection, preprocessing, metrics\n","from sklearn.metrics import mean_squared_error, r2_score, make_scorer\n","from sklearn.preprocessing import scale, StandardScaler, OneHotEncoder\n","from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, cross_validate\n","from sklearn.pipeline import Pipeline, make_pipeline\n","from sklearn.compose import ColumnTransformer, make_column_transformer\n","\n","import lightgbm as lgb\n","from sklearn.linear_model import LinearRegression, Lasso, Ridge, ElasticNet\n","from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n","from sklearn.decomposition import PCA\n","import sklearn\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","from feature_selector import FeatureSelector\n","\n","#XGBoost libraries\n","import xgboost as xgb"]},{"cell_type":"markdown","metadata":{"id":"Iw1_Se8hQ-l3"},"source":["<a id=\"32\"></a>\n","#### b. Version of Installed Python Library"]},{"cell_type":"markdown","metadata":{"id":"QJoUQutYQ-l4"},"source":["For the purpose of reproducibility of this analysis, displaying the versions of installed libraries"]},{"cell_type":"code","source":["import pandas as pd\n","\n","def _append(self, other, ignore_index=False, verify_integrity=False, sort=False):\n","    from pandas import concat, DataFrame\n","    if isinstance(other, dict): other = DataFrame([other])\n","    elif not isinstance(other, DataFrame): other = DataFrame(other)\n","    return concat([self, other], ignore_index=ignore_index, verify_integrity=verify_integrity, sort=sort)\n","\n","pd.DataFrame.append = _append\n"],"metadata":{"id":"a9c8g-s6oCP7","executionInfo":{"status":"aborted","timestamp":1754631596635,"user_tz":-330,"elapsed":15635,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P7bPuU1RQ-l4","executionInfo":{"status":"aborted","timestamp":1754631596637,"user_tz":-330,"elapsed":15631,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["print('matplotlib: {}'.format(matplotlib.__version__))\n","print('sklearn: {}'.format(sklearn.__version__))\n","print('seaborn: {}'.format(sns.__version__))\n","print('pandas: {}'.format(pd.__version__))\n","print('numpy: {}'.format(np.__version__))\n","print('xgboost: {}'.format(xgb.__version__))\n"]},{"cell_type":"markdown","metadata":{"id":"MziX_aFWQ-l6"},"source":["<a id=\"33\"></a>\n","#### c. Loading Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kei9O1XHQ-l7","executionInfo":{"status":"aborted","timestamp":1754631596638,"user_tz":-330,"elapsed":15631,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["DATA_PATH = 'data'\n","FILE_NAME = 'recs2009_public.csv'\n","df = pd.read_csv(os.path.join(DATA_PATH, FILE_NAME), encoding = 'utf-8-sig')"]},{"cell_type":"markdown","metadata":{"id":"yULyR8i7Q-l7"},"source":["<a id=\"4\"></a>\n","### 4. Exploratory Data Analysis (EDA)\n","\n","In the EDA phase, we will perform initial investigations on the dataset to check the following:\n","- data dimensionality (or shape) alongwith observing first few observations of the dataset\n","- data types (whether categorical or numerical)\n","- generate summary statistics\n","- missing values\n","- explore our target variable _'KWH'_ and its possible predictors from the dataset\n","- detect outliers and anomalies\n","- explore additional predictor variables"]},{"cell_type":"markdown","metadata":{"id":"pQew6Vy4Q-l8"},"source":["<a id=\"41\"></a>\n","#### a. Data Dimensionality"]},{"cell_type":"code","source":["import pandas as pd\n","\n","# Patch for broken IPython display in Colab\n","if not hasattr(pd.Index, '_format_flat'):\n","    pd.Index._format_flat = lambda self, include_name=False: self.tolist()\n"],"metadata":{"id":"XOsrrNAIo1CO","executionInfo":{"status":"aborted","timestamp":1754631596639,"user_tz":-330,"elapsed":15632,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z8MPpbgdQ-l8"},"source":["Let's start by glancing first few observations of the RECS dataset alongwith its dimensionality"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1MI-Jb9IQ-l9","executionInfo":{"status":"aborted","timestamp":1754631596789,"user_tz":-330,"elapsed":1,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["# Set pandas to display all columns when printing DataFrames\n","# df.shape[1] gives the number of columns in the DataFrame\n","pd.options.display.max_columns = df.shape[1]\n","\n","\n","# Display the first 5 rows of the DataFrame\n","# Useful for getting a quick look at the data: column names, sample values, data types, etc.\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P-_vJlb3Q-l-","executionInfo":{"status":"aborted","timestamp":1754631596791,"user_tz":-330,"elapsed":15770,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["print(f\"Shape of the dataset df_recs is {df.shape}\") #rows, columns"]},{"cell_type":"markdown","metadata":{"id":"qV7X3vPWQ-l_"},"source":["As can be seen above, RECS 2009 dataset consists of 12,083 observations and 940 features. Let's explore how many of these features are categorical vs numerical"]},{"cell_type":"markdown","metadata":{"id":"VnoYJLiXQ-mA"},"source":["<a id=\"42\"></a>\n","#### b. Data Types"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nWcpowC2Q-mA","executionInfo":{"status":"aborted","timestamp":1754631596803,"user_tz":-330,"elapsed":15773,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["num_features = df.dtypes[df.dtypes != \"object\"].index\n","print(\"Number of Numerical features: \", len(num_features))\n","\n","cat_features = df.dtypes[df.dtypes == \"object\"].index\n","print(\"Number of Categorical features: \", len(cat_features))"]},{"cell_type":"markdown","metadata":{"id":"XcPgwlyPQ-mB"},"source":["`df.info()` method prints a concise summary of a dataframe including count of index data type, number of columns. `df.describe()` method prints out the descriptive statistics including mean, median (i.e. Second Quartile, Q2 depicted by 50% in the summary table below), standard deviation, range, Q1 and Q3 quartile values. As can be seen below, RECS 2009 dataset consists of three different column data types:\n","- _float64_ data type with 50 columns\n","- _int64_ data type with 885 columns, and\n","- _object_ data type with 5 columns\n","\n","The count of column data types is consistent with the count of numerical and categorical features we calculated above. Count of Numerical features we calculated above is actually sum of number of _float64_ and _int64_ data type columns i.e. (50 + 885 = 935).\n","\n","The number of records, 12083 is consistent with what we found using `df.shape`. We don't have any NA values in the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"891bZc7gQ-mB","executionInfo":{"status":"aborted","timestamp":1754631596805,"user_tz":-330,"elapsed":15770,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":[" df.info()"]},{"cell_type":"markdown","metadata":{"id":"QGE_jHMIQ-mC"},"source":["<a id=\"43\"></a>\n","#### c. Summary Statistics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ecwo4gKhQ-mC","executionInfo":{"status":"aborted","timestamp":1754631596806,"user_tz":-330,"elapsed":15765,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.describe()"]},{"cell_type":"markdown","metadata":{"id":"Vehk4opyQ-mD"},"source":["**Key Observations:**\n","- Mean value is different (i.e. more or less) than median value for columns HDD65, CDD65, HDD30YR, CDD30YR, TOTSQFT, KWH, CDD80, OA_LAT\n","- Large difference in 75th percentile and maximum value for columns HDD65, CDD65, HDD30YR, CDD30YR, TOTSQFT, KWH, CDD80, OA_LAT\n","- Thus, these observations mean that there are outier values in our dataset"]},{"cell_type":"markdown","metadata":{"id":"lbY153wdQ-mD"},"source":["Now, let's check the summary statistics for categorical features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8IrKPF0oQ-mD","executionInfo":{"status":"aborted","timestamp":1754631596906,"user_tz":-330,"elapsed":15857,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.describe(include=['object'])"]},{"cell_type":"markdown","metadata":{"id":"QHMWMZzeQ-mF"},"source":["In the table above, we can see the number of unique values as well as the top value and it's frequency for all the categorical features. Upon closely looking at features 'NOCRCASH' and 'NKRGALNC', we can observe that the topmost value is -2 with a frequency of 9,958 in each of these features. This doesn't seem right as -2 is of _int64_ data type and we previously found these features to be categorical in nature. Hence we will now check the counts of unique values for each of these features to see if there was any data entry error"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrXxJAFaQ-mG","executionInfo":{"status":"aborted","timestamp":1754631596940,"user_tz":-330,"elapsed":15868,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df['NOCRCASH'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wEll5LVtQ-mH","executionInfo":{"status":"aborted","timestamp":1754631596941,"user_tz":-330,"elapsed":15854,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df['NKRGALNC'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"DQxmItjDQ-mH"},"source":["We can observe from the above value counts for features 'NOCRCASH' and 'NKRGALNC' that there is '.' (period symbol) in two observations for each of these features. We assume that these two observations with '.' value might be a data error while recording and saving the data.\n","\n","In addition, it can be noticed that the value '-2' appears twice with a frequency count of 9958 and 2028 in features 'NOCRCASH' and 'NKRGALNC'. There might be a trailing or leading space after -2 value and hence we see -2 twice for each of the variables. The actually count of -2 i.e. Not Applicable value in each of these features is 11986 (9958 + 2028), which is more than 99% of the observations in the RECS 2009 dataset. Since more than 99% of the observations in 'NOCRCASH' and 'NKRGALNC' are actually 'Not Applicable' values, hence we can conclude that there is not enough variance in values of features 'NOCRCASH' and 'NKRGALNC' and thus these features would have no predicitve power over the outcome variable and we can safely drop these features from the dataset in the data preprocessing section."]},{"cell_type":"markdown","metadata":{"id":"8uf-xa5rQ-mH"},"source":["Let's check the unique value counts for the remaining categorical features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7jgIwGtvQ-mI","executionInfo":{"status":"aborted","timestamp":1754631596945,"user_tz":-330,"elapsed":15858,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["cat_features_remaining = ['METROMICRO', 'UR', 'IECC_Climate_Pub']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m2YbllwDQ-mI","executionInfo":{"status":"aborted","timestamp":1754631596947,"user_tz":-330,"elapsed":15853,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["for col in cat_features_remaining:\n","    print(df[col].value_counts(), '\\n')"]},{"cell_type":"markdown","metadata":{"id":"yaukS_mdQ-mJ"},"source":["We can also check how values in each of the categorical features are distributed using argument `normalize = True` inside method `value_counts()` distribution of the unique value counts"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HMWsS0DJQ-mJ","executionInfo":{"status":"aborted","timestamp":1754631596949,"user_tz":-330,"elapsed":15851,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["for col in cat_features_remaining:\n","    print(round(df[col].value_counts(normalize= True)*100), '\\n')"]},{"cell_type":"markdown","metadata":{"id":"JMlttutDQ-md"},"source":["85% of the responses (n = 10302) were recorded from housing units in census metropolitan area where as 9% of the responses (n = 1109) were recorded from housing units in census micropolitan area. On the contrary, 80% of the surveyed housing units (n = 9656) were from urban area whereas rest 20% (n = 2427) were from rural area"]},{"cell_type":"markdown","metadata":{"id":"MjQlMn1iQ-me"},"source":["<a id=\"44\"></a>\n","#### d. Check for Missing Values"]},{"cell_type":"markdown","metadata":{"id":"rNsOHHv0Q-me"},"source":["Next, we check dataset for any missing values in it"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TdIL7Me0Q-me","executionInfo":{"status":"aborted","timestamp":1754631596950,"user_tz":-330,"elapsed":15847,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.isnull().sum().sum()"]},{"cell_type":"markdown","metadata":{"id":"n6C1gGyIQ-mf"},"source":["Upon checking dataset for null values using method `df.isnull()`, we did not find any missing values in the dataset. However, looking into the RECS 2009 survey codebook, it was observed that most of the features had a response code of -2 i.e. 'Not Applicable' which meant that the feature being measured doesn't apply to survey respondent's housing, consumption and expenditure characterstics. Hence, we will now check how many features in total were marked -2 i.e. 'Not Applicable' by majority of the survey respondents\n","\n","Let's check how many features in total were marked -2 i.e. 'Not Applicable' in more than 95% of the observations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YqUp2CFDQ-mh","executionInfo":{"status":"aborted","timestamp":1754631596952,"user_tz":-330,"elapsed":15845,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df_exclude_object_dtype = df.select_dtypes(exclude=['object'])\n","na_col_names = df_exclude_object_dtype.columns[((df_exclude_object_dtype == -2).sum()) > round(df.shape[0]*0.95)]\n","print('{} features were marked as -2 i.e. \"Not Applicable\" by majority (95%) of the respondents'.format(len(na_col_names)),\"\\n\" \"Here's the name of the features:\", na_col_names)"]},{"cell_type":"markdown","metadata":{"id":"aLtXjeELQ-mh"},"source":["72 features were found to have NA i.e. 'Not Applicable' values in more than 95% of the observations. Since, these features have majority of the data values as 'NA', it can be safely concluded that these features would have negligible or almost no predictive power over the outcome variable. In the later part of this section, we will check if these features correlate with our target variable _'KWH'_ in order to make a decision as to whether these features are to be retained or dropped from final set of feature dataset"]},{"cell_type":"markdown","metadata":{"id":"SXLCVKa9Q-mh"},"source":["<a id=\"45\"></a>\n","#### e. Explore Target Variable and Extract Important Features"]},{"cell_type":"markdown","metadata":{"id":"v5fbdYdjQ-mh"},"source":["Let's explore our target variable _'KWH'_ and extract important features which are highly correlated with the target variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xnqik8rDQ-mi","executionInfo":{"status":"aborted","timestamp":1754631596953,"user_tz":-330,"elapsed":15840,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df['KWH'].describe()"]},{"cell_type":"markdown","metadata":{"id":"1m-TZkNlQ-mi"},"source":["Few key insights by looking at summary statistics of the dependent variable are as follows:\n","- Target variable _'KWH'_ is numeric in nature\n","- The mean value of target variable is ~ 11288 KWH and a standard deviation of ~ 7641 KWH which means the distance between observations and the mean value of KWH is considerably higher\n","- 50% of the KWH values lies between 17 - 9623 KWH, whereas 75% the values lies between 17 - 14765 KWH\n","- The min and max value for target variable are 17 and 150254 KWH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qdsfkLhnQ-mi","executionInfo":{"status":"aborted","timestamp":1754631596954,"user_tz":-330,"elapsed":15835,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["fig, ax = plt.subplots(figsize=[10,6])\n","ax.set_xlim(0,90000)\n","sns.distplot(df['KWH'],ax=ax, bins=100).set(title = 'KWH Distribution')"]},{"cell_type":"markdown","metadata":{"id":"sw3X0z8rQ-mi"},"source":["It can be seen from the graph above that the distribution of target variable is positively skewed. This means that the outliers of the distribution are further towards the right. Let's create box plot (i.e. box and whisker diagram) for our target variable to see the range of values in a more intutive way"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wsGL2EUSQ-mj","executionInfo":{"status":"aborted","timestamp":1754631596956,"user_tz":-330,"elapsed":15831,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["sns.set(style=\"darkgrid\")\n","fig, ax = plt.subplots(figsize = (10,6))\n","ax = sns.boxplot(x=df['KWH']).set(title = 'KWH Boxplot')"]},{"cell_type":"markdown","metadata":{"id":"FE7h1SUVQ-mj"},"source":["As previously observed from the distribution graph of target variable, we can clearly see in the box plot above that almost all the observations for energy consumption are under 80,000 KWH except for just one extreme outlier value above KWH 140,000. We will take care of the outlier values in the data preprocessing section\n","\n","In the distribution plot above for our target variable, we observed that the distribution was right or positively skewed. Hence, to tackle issue of skewness, let's apply the log transformation on our target variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wGtAnYmFQ-mj","executionInfo":{"status":"aborted","timestamp":1754631596960,"user_tz":-330,"elapsed":15828,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df['KWHlog'] = np.log(df['KWH'])\n","fig, ax = plt.subplots(figsize=[10,6])\n","sns.distplot(df['KWHlog'],ax=ax).set(title = 'KWHlog Distribution')"]},{"cell_type":"markdown","metadata":{"id":"UiNt3dbQQ-mk"},"source":["We can observe from the distribution graph above that log transformation has brought the distribution close to normal or bell shape curve. In other words, we can say that the log transformation reduced the skeness of our original data and hence the statistical analysis results from log transformed data will be more accurate or valid."]},{"cell_type":"markdown","metadata":{"id":"lCobRg_fQ-mk"},"source":["To extract important predictors of our target variable 'KWH', we will find correlation between target variable 'KWH' and all predictor variables and filter the predictor variables by keeping the threshold correlation value of abs(0.4), meaning the predictor variables with a correlation of abs(0.4) with target variable 'KWH' will be shown."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FA1Atb0cQ-mk","executionInfo":{"status":"aborted","timestamp":1754631597012,"user_tz":-330,"elapsed":2,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["# Select only numeric columns from the DataFrame\n","df_numeric = df.select_dtypes(include=['number'])\n","\n","# Compute the correlation matrix\n","corr = df_numeric.corr()\n","\n","# Compute absolute correlation with the target variable 'KWH'\n","corr_target = abs(corr['KWH'])\n","\n","# Select features with correlation > 0.4\n","relevant_features = corr_target[corr_target > 0.4]\n","\n","# Sort features by correlation value\n","relevant_features = relevant_features.sort_values(ascending=False)\n","\n","# Print summary\n","print(\"{} features were found to have correlation value of 0.4 or more with our target variable 'KWH'\".format(len(relevant_features)))\n","print('----------------------------------------------------------------------------------------')\n","print('These features are: \\n{}'.format(list(relevant_features.index)))\n"]},{"cell_type":"markdown","metadata":{"id":"xpqH6AG3Q-mk"},"source":["Let's now check which are the top 20 predictors for tagret variable 'KWH' on the basis of the results of correlation analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zZlnCsfcQ-ml","executionInfo":{"status":"aborted","timestamp":1754631597015,"user_tz":-330,"elapsed":4,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["relevant_features.sort_values(ascending = False)[:21]"]},{"cell_type":"markdown","metadata":{"id":"Ua6M4r5LQ-ml"},"source":["Let's plot the 'KWH' correlation matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2FmlYaXTQ-mm","executionInfo":{"status":"aborted","timestamp":1754631597019,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# Step 1: Keep only numeric columns\n","df_numeric = df.select_dtypes(include=['number'])\n","\n","# Step 2: Select top 33 features most correlated with 'KWH'\n","k = 33\n","cols = df_numeric.corr().nlargest(k, 'KWH')['KWH'].index\n","\n","# Step 3: Compute correlation matrix for selected features\n","cm = df_numeric[cols].corr()\n","\n","# Step 4: Plot heatmap\n","plt.figure(figsize=(30,18))\n","sns.heatmap(cm, annot=True, cmap='viridis')\n","plt.title(f\"Top {k} Features Correlated with KWH\", fontsize=20)\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"54FxIfKgQ-mm"},"source":["Following may be observed from the above graph:\n","- green and yellow color represents strong positive correlation whereas blue and purple color represents very weak positive or negative correlation\n","- KWH has moderate to strong positive correlation (> 0.4) with all top 32 important predictors\n","- Many predictor variables have high correlation with each other (such as BTUELCOL, KWHCOL, TOTALBTUCOL), thus indicating the presence of multicollinearity among predictor variables/features. We will explore collinearity among predictor features in detail in the feature selection section\n","\n","Let's now explore how does these predictor variables affect the target variable 'KWH' by graphing a scatter plot between target variable and a few predictor variables\n","\n","**How does 'BTUEL' i.e. Electricity Usage in BTU relates with the target variable 'KWH'?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uhMRrbtpQ-mm","executionInfo":{"status":"aborted","timestamp":1754631597021,"user_tz":-330,"elapsed":4,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["plt.figure(figsize=(10,10))\n","plt.scatter(x='BTUEL',y='KWH',data=df)\n","ax.set_ylim(ymin=0)\n","plt.xlabel('Electricity Usage in BTU')\n","plt.ylabel('Electricity Usage in KWH')"]},{"cell_type":"markdown","metadata":{"id":"RS77pOTGQ-mn"},"source":["We can see from the scatterplot that KWH and BTUEL is perfectly correlated. Infact, it is a duplicate variable indicating total electricity usage in different units, thousand BTU in variable BTUEL and kilowatt-hours in variable 'KWH'. Hence, in the data prepracessing section we will remove these duplicates. There are few other features in the RECS dataset which are actually either duplicates or calculated by summing up the one or more predictor features. These are as follows:\n","\n","- KWH = KWHSPH + KWHCOL + KWHWTH + KWHRFG + KWHOTH<br>\n","- BTUEL = BTUELSPH + BTUELCOL + BTUELWTH + BTUELRFG + BTUELOTH<br>\n","- DOLLAREL = DOLELSPH + DOLELCOL + DOLELWTH + DOLELRFG + DOLELOTH<br>\n","- CUFEETNG = CUFEETNGSPH + CUFEETNGWTH + CUFEETNGOTH<br>\n","- BTUNG = BTUNGSPH + BTUNGWTH + BTUNGOTH<br>\n","- DOLLARNG = DOLNGSPH + DOLNGWTH + DOLNGOTH<br>\n","- GALLONLP = GALLONLPSPH + GALLONLPWTH + GALLONLPOTH<br>\n","- BTULP = BTULPSPH + BTULPWTH + BTULPOTH<br>\n","- DOLLARLP = DOLLPSPH\t+ DOLLPWTH + DOLLPOTH<br>\n","- GALLONFO = GALLONFOSPH + GALLONFOWTH + GALLONFOOTH<br>\n","- BTUFO = BTUFOSPH + BTUFOWTH + BTUFOOTH<br>\n","- DOLLARFO = DOLFOSPH + DOLFOWTH + DOLFOOTH<br>\n","- GALLONKER = GALLONKERSPH + GALLONKERWTH + GALLONKEROTH<br>\n","- BTUKER = BTUKERSPH + BTUKERWTH + BTUKEROTH<br>\n","- DOLLARKER = DOLKERSPH + DOLKERWTH + DOLKEROTH<br>\n","- TOTALBTU = TOTALBTUSPH + TOTALBTUCOL + TOTALBTUWTH + TOTALBTURFG + TOTALBTUOTH<br>\n","- TOTALDOL = TOTALDOLSPH + TOTALDOLCOL + TOTALDOLWTH + TOTALDOLRFG + TOTALDOLOTH<br>\n","- TOTALBTUSPH = BTUELSPH + BTUNGSPH + BTULPSPH + BTUFOSPH + BTUKERSPH<br>\n","- TOTALBTUCOL = BTUELCOL<br>\n","- TOTALBTUWTH = BTUELWTH + BTUNGWTH + BTULPWTH +  + BTUFOWTH + BTUKERWTH<br>\n","- TOTALBTURFG = BTUELRFG<br>\n","- TOTALBTUOTH = BTUELOTH + BTUNGOTH + BTULPOTH + BTUFOOTH + BTUKEROTH<br>\n","- TOTALDOLSPH = DOLELSPH + DOLNGSPH + DOLLPSPH + DOLFOSPH + DOLKERSPH<br>\n","- TOTALDOLCOL = DOLELCOL<br>\n","- TOTALDOLWTH = DOLELWTH + DOLNGWTH + DOLLPWTH + DOLFOWTH + DOLKERWTH<br>\n","- TOTALDOLRFG = DOLELRFG<br>\n","- TOTALDOLOTH = DOLELOTH + DOLNGOTH + DOLLPOTH + DOLFOOTH + DOLKEROTH<br>\n","\n","We will talk about these in detail in data preprocessing section\n"]},{"cell_type":"markdown","metadata":{"id":"zx2yoFeuQ-mn"},"source":["**Now, let's check how does Total Cooled Square Footage (TOTCSQFT), Total heated square footage (TOTHSQFT) and Total Number of Rooms in Housing (TOTROOMS) relates with the target variable 'KWH'?**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L9UICmO6Q-mn","executionInfo":{"status":"aborted","timestamp":1754631597022,"user_tz":-330,"elapsed":4,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["#f = plt.figure(figsize = (10,3))\n","fig, axs = plt.subplots(1, 3, figsize=(24, 8))\n","axs[0].scatter(x='TOTCSQFT',y='KWH',data=df)\n","axs[1].scatter(x='TOTHSQFT',y='KWH',data=df)\n","axs[2].scatter(x='TOTROOMS',y='KWH',data=df)\n","axs[0].set_xlabel('Total Cooled Square Footage')\n","axs[0].set_ylabel('Electricity Usage in KWH')\n","axs[1].set_xlabel('Total Heated Square Footage')\n","axs[1].set_ylabel('Electricity Usage in KWH')\n","axs[2].set_xlabel('Total Rooms in Housing')\n","axs[2].set_ylabel('Electricity Usage in KWH')"]},{"cell_type":"markdown","metadata":{"id":"H8dQWNkPQ-mn"},"source":["We previosuly found that total cooled square footage, total heated square footage and total rooms in housing had a moderate positive correlation with target variable 'KWH'. The scatter plots above are quite informative and are in-line with the results from correlation analysis. We can see that we do not have one fixed linear relationship across the entire domain of values of total cooled square footage, total heated square footage and total rooms in housing."]},{"cell_type":"markdown","metadata":{"id":"ufSIIJ8VQ-mo"},"source":["<a id=\"46\"></a>\n","#### f. Detect Outliers and Anomalies"]},{"cell_type":"markdown","metadata":{"id":"H-urUUF4Q-mo"},"source":["Previously, in the summary statistics section, we found that the mean value was different from the median value for the features HDD65, CDD65, HDD30YR, CDD30YR, TOTSQFT, KWH, CDD80, OA_LAT. In addition, we found a large difference in the 75th percentile and maximum value for these features. These observations indicates the presence of potential outlier values in these features. Let's check each of these feature for outliers using box plot (i.e. box and whisker diagram)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rS8oo0XjQ-mo","executionInfo":{"status":"aborted","timestamp":1754631597024,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["outl_cols = ['HDD65', 'CDD65', 'HDD30YR', 'CDD30YR', 'TOTSQFT', 'CDD80', 'OA_LAT']\n","number_of_columns=len(outl_cols)\n","number_of_rows = len(outl_cols)-1/number_of_columns\n","plt.figure(figsize=(2*number_of_columns,5*number_of_rows))\n","for i in range(0,len(outl_cols)):\n","    plt.subplot(round(number_of_rows + 1),number_of_columns,i+1)\n","    sns.set_style('whitegrid')\n","    sns.boxplot(y = df[outl_cols[i]],orient='h')\n","    plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"cQnSU5ocQ-mo"},"source":["We can see from the box plot above that almost all the features shows outliers present in the dataset. Let's now check the linearity of the variables by plotting distribution graph and look for skewness of features using Kernel density estimate (kde)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TufdE0TmQ-mo","executionInfo":{"status":"aborted","timestamp":1754631597026,"user_tz":-330,"elapsed":7,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["plt.figure(figsize=(2*number_of_columns,5*number_of_rows))\n","for i in range(0,len(outl_cols)):\n","    plt.subplot(round(number_of_rows + 1),number_of_columns,i+1)\n","    sns.distplot(df[outl_cols[i]],kde=True)\n","    plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"rjr6McaHQ-mp"},"source":["All the predictor variables depicted in kde graph above are right skewed/positively skewed\n","\n","In the [Check for Missing Values](#d-check-for-missing-values) section, we found 72 predictor features that had NA i.e. 'Not Applicable' values in more than 95% of the observations. Let's now check whther these 72 features correlate with the target variable 'KWH' or not"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7ciH5gReQ-mp","executionInfo":{"status":"aborted","timestamp":1754631597050,"user_tz":-330,"elapsed":22,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["target_var = 'KWH'\n","feature_target_corr = {}\n","for col in list(na_col_names.values):\n","    feature_target_corr[col + '-' + target_var] = round(scipy.stats.spearmanr(df[col], df[target_var])[0],2)\n","print(\"Predictor Feature vs Target Variable Correlations\")\n","print(feature_target_corr)"]},{"cell_type":"markdown","metadata":{"id":"g6mkJHFMQ-mp"},"source":["Spearman correlation coefficient values between target variable 'KWH' and 72 predictor features reveal low to negligible correlation. We could have employed imputation technique to address these 72 features, however, we can drop them due to high missing values i.e. 'Not Applicable' values and evidence of negligible correlation suggesting that the set of these features have negligible amount of predictive power over the outcome variable. Hence, these features will be removed in data preprocessing section"]},{"cell_type":"markdown","metadata":{"id":"OYn25ZZFQ-mp"},"source":["<a id=\"47\"></a>\n","#### g. Explore Additional Predictor Variables"]},{"cell_type":"markdown","metadata":{"id":"Uf3u3BTwQ-mq"},"source":["Let's now explore the relationship between target variable 'KWH' and categorical features 'METROMICRO', 'UR' and 'IECC_Climate_Pub' using box plots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bn01K8nPQ-mq","executionInfo":{"status":"aborted","timestamp":1754631597052,"user_tz":-330,"elapsed":15867,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["number_of_columns=len(cat_features_remaining)\n","number_of_rows = len(cat_features_remaining)-1/number_of_columns\n","plt.figure(figsize=(10*number_of_columns, 16*number_of_rows))\n","for i in range(0,len(cat_features_remaining)):\n","    plt.subplot(round(number_of_rows + 1),number_of_columns,i+1)\n","    sns.set_style('whitegrid')\n","    sns.boxplot(x = df[cat_features_remaining[i]], y ='KWH', data = df)\n","    plt.xlabel(cat_features_remaining[i], size = 20)\n","    plt.ylabel('KWH', size = 20)\n","    plt.xticks(size = 20)\n","    plt.yticks(size = 20)\n","    plt.tight_layout()"]},{"cell_type":"markdown","metadata":{"id":"up9462ODQ-mq"},"source":["We can observe in the box plots above that all the categories of categorical features has energy consumption value under KWH 80,000 except for just one extreme outlier value above KWH 140,000. We will take care of the outlier values in the data preprocessing section\n","\n","We will convert the categorical labels of categorical features into numbers using the `OneHotEncoder` inside `ColumnTransformer` later in the model building section. It is to be noted that incase the cardinality of a particular categorical feature is very high, then using one-hot encoding is not recommended as it might lead to a curse of dimensionality. However, in our dataset we have three categorical features with feature 'IECC_Climate_Pub' having the maximum unique feature values of 11\n","\n","Let's now explore how some of the housing characterstics, usage patterns and household demographics features are related with the target variable 'KWH' i.e. Total Electricity usage in KWH. The following features under housing characterstics, usage patterns and household demographics will be explored in-depth to uncover underlying patterns in the dataset:\n","- **Housing Characterstics:** type of housing unit (TYPEHUQ), year housing unit was built (YEARMADERANGE), total number of rooms in housing (TOTROOMS)\n","- **Usage Patterns:** frequency of clothes dryer use (DRYRUSE), frequency of oven use (OVENUSE), frequency of dishwasher use (DWASHUSE)\n","- **Household Demographics :** number of household members (NHSLDMEM), gross household income (MONEYPY)\n","\n","It may be noted here that all the features falling under housing characterstics, usage patters and household demographics are discrete numerical variables i.e. the variables whose values exist in a particular range or are countable in a finite amount of time\n","\n","**How does Housing Characterstics features relates with the target variable 'KWH'?**\n","\n","a) Relation Between Type of Housing Unit and Energy Consumption in KWH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wy5LBclaQ-mq","executionInfo":{"status":"aborted","timestamp":1754631597054,"user_tz":-330,"elapsed":15864,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["# Imports\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","\n","# Plot\n","plt.figure(figsize=(20,6))\n","\n","# 1. Countplot\n","plt.subplot(1,3,1)\n","sns.set_style('whitegrid')\n","ax0 = sns.countplot(x='TYPEHUQ', data=df)\n","ax0.set_title('Distribution of Housing Unit')\n","\n","# 2. Boxplot\n","plt.subplot(1,3,2)\n","ax1 = sns.boxplot(x='TYPEHUQ', y='KWH', data=df)\n","ax1.set_title('Relationship b/w Housing Unit Type & Energy Consumption')\n","\n","# 3. Barplot (Median)\n","plt.subplot(1,3,3)\n","df.groupby('TYPEHUQ')['KWH'].median().plot.bar()\n","plt.title('Median KWH for each Housing Unit Type')\n","plt.ylabel('KWH')\n","\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"markdown","metadata":{"id":"BRp7KupTQ-mr"},"source":["Based on the above plots, we can observe the following:\n","- Housing unit type 2 i.e. Single-Family Detached dominates the data distribution of Housing Unit type (n = ~7800 observations)\n","- We may observe a clear pattern in the boxplot above that as housing unit type change from Mobile home to single-family home to apartment in building, lesser the energy is consumed\n","- From the plot 'Median KWH for each Housing Unit type', we can see that feature TYPEHUQ has a direct relation with the target variable"]},{"cell_type":"markdown","metadata":{"id":"koJ03mEYQ-mr"},"source":["b) Relation Between Year in which Housing Unit was built and Energy Consumption in KWH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CqcLMA6qQ-mr","executionInfo":{"status":"aborted","timestamp":1754631597055,"user_tz":-330,"elapsed":15859,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["plt.figure(figsize=(20,6))\n","plt.subplot(1,3,1)\n","sns.set_style('whitegrid')\n","ax0 = sns.countplot(x='YEARMADERANGE', data = df)\n","ax0.set_title('Distribution of Housing Unit Built Year')\n","plt.subplot(1,3,2)\n","ax1 = sns.boxplot(x='YEARMADERANGE', y ='KWH', data = df)\n","ax1.set_title('Relationship b/w Housing Unit Built Year & Energy Consumption')\n","plt.subplot(1,3,3)\n","df.groupby('YEARMADERANGE')['KWH'].median().plot.bar()\n","plt.title('Median KWH for each Housing Unit Built Year Range')\n","plt.ylabel('KWH')"]},{"cell_type":"markdown","metadata":{"id":"sTi7Z5EAQ-mr"},"source":["Based on the above plots, we can observe the following:\n","- We can see from the barplot that Housing Unit Built Year has data well-distributed across different levels/range of built year\n","- We may see a clear pattern in the boxplot above that most recently the housing unit was built, more energy consumption can be observed\n","- 'Median KWH for each housing unit built year range' graph observation is in-line with boxplot results indicating that feature YEARMADERANGE indeed has a direct relation with the target variable"]},{"cell_type":"markdown","metadata":{"id":"oKKvCaSXQ-ms"},"source":["c) Relation Between Total Rooms in  Housing and Energy Consumption in KWH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVHad-vMQ-ms","executionInfo":{"status":"aborted","timestamp":1754631597057,"user_tz":-330,"elapsed":15856,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["plt.figure(figsize=(27,6))\n","plt.subplot(1,3,1)\n","sns.set_style('whitegrid')\n","ax0 = sns.countplot(x='TOTROOMS', data = df)\n","ax0.set_title('Distribution of Total Rooms in Housing')\n","plt.subplot(1,3,2)\n","ax1 = sns.boxplot(x='TOTROOMS', y ='KWH', data = df)\n","ax1.set_title('Relationship b/w Housing Unit Total Rooms & Energy Consumption')\n","plt.subplot(1,3,3)\n","df.groupby('TOTROOMS')['KWH'].median().plot.bar()\n","plt.title('Median KWH for # Rooms in Housing')\n","plt.ylabel('KWH')"]},{"cell_type":"markdown","metadata":{"id":"HODJlh1iQ-ms"},"source":["Based on the above plots, we can observe the following:\n","- Most of the Housing units typically have less than 12 rooms in total. Only few of the housings have more than or equal to 12 rooms in total. Hence, we could combine these levels together and create a new one 'more than 11 rooms'. We will take care of this in data preprocessing section\n","- We may see a clear pattern in the boxplot above that more the number of rooms in the housing, more the energy consumption is\n","- Similarly, the median KWH plot also show that as number of rooms in housing increases, the energy consumption increases as well, indicating that feature TOTROOMS has a direct relation with the target variable"]},{"cell_type":"markdown","metadata":{"id":"zM5cjlG4Q-ms"},"source":["**How does Appliance Usage Pattern features relates with the target variable 'KWH'?**\n","\n","a) Relation Between Frequency of Clothes Dryer Use and Energy Consumption in KWH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jnH_Y2O1Q-mt","executionInfo":{"status":"aborted","timestamp":1754631597059,"user_tz":-330,"elapsed":15851,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["plt.figure(figsize=(20,6))\n","plt.subplot(1,3,1)\n","sns.set_style('whitegrid')\n","ax0 = sns.countplot(x='DRYRUSE', data = df)\n","ax0.set_title('Distribution of Frequency of Clothes Dryer Use')\n","plt.subplot(1,3,2)\n","ax1 = sns.boxplot(x='DRYRUSE', y ='KWH', data = df)\n","ax1.set_title('Relationship b/w Frequency of Clothes Dryer Use & Energy Consumption')\n","plt.subplot(1,3,3)\n","df.groupby('DRYRUSE')['KWH'].median().plot.bar()\n","plt.title('Median KWH for Frequency of Clothes Dryer Use')\n","plt.ylabel('KWH')"]},{"cell_type":"markdown","metadata":{"id":"alhIUMuhQ-mt"},"source":["Based on the above plots, we can observe the following:\n","- Most of the households use clothes dryer everytime they wash clothes (depicted by factor/level value 1. Note that factor/level value 3 indicates infrequent use of clothes dryer)\n","- We can observe a clear pattern in the boxplot above that more frequently a household use clothes dryer, more is the energy consumption\n","- The median KWH plot results are in-line with boxplot results, indicating that more frequently household use clothes dryer, more is the median value of energy consumption. Thus, feature DRYRUSE has a direct relation with the target variable"]},{"cell_type":"markdown","metadata":{"id":"TmNZPk_LQ-mt"},"source":["b) Relation Between Frequency of Oven Use and Energy Consumption in KWH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HFnj7TevQ-mt","executionInfo":{"status":"aborted","timestamp":1754631597078,"user_tz":-330,"elapsed":15863,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["plt.figure(figsize=(20,6))\n","plt.subplot(1,3,1)\n","sns.set_style('whitegrid')\n","ax0 = sns.countplot(x='OVENUSE', data = df)\n","ax0.set_title('Distribution of Frequency of Oven Use')\n","plt.subplot(1,3,2)\n","ax1 = sns.boxplot(x='OVENUSE', y ='KWH', data = df)\n","ax1.set_title('Relationship b/w Frequency of Oven Use & Energy Consumption')\n","plt.subplot(1,3,3)\n","df.groupby('OVENUSE')['KWH'].median().plot.bar()\n","plt.title('Median KWH for Frequency of Oven Use')\n","plt.ylabel('KWH')"]},{"cell_type":"markdown","metadata":{"id":"wHezNx_GQ-mu"},"source":["Based on the above plots, we can observe the following:\n","- Majority of the households use the oven 'few times a week' (depicted by factor/level value 1. Note that factor/level value 1 indicates use of oven three or four times a day)\n","- Contrary to the results of barplot, from the boxplot and median KWH plot, we can observe that most energy consumption was from the households who were using oven 'two times a day' (depicted by higher median value for factor/level value 2 of feature OVENUSE)"]},{"cell_type":"markdown","metadata":{"id":"08L0Pkv1Q-mu"},"source":["c) Relation Between Frequency of Dishwasher Use and Energy Consumption in KWH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FfhSPB3Q-mu","executionInfo":{"status":"aborted","timestamp":1754631597080,"user_tz":-330,"elapsed":15859,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["plt.figure(figsize=(20,6))\n","plt.subplot(1,3,1)\n","sns.set_style('whitegrid')\n","ax0 = sns.countplot(x='DWASHUSE', data = df)\n","ax0.set_title('Distribution of Frequency of Dishwasher Use')\n","plt.subplot(1,3,2)\n","ax1 = sns.boxplot(x='DWASHUSE', y ='KWH', data = df)\n","ax1.set_title('Relationship b/w Frequency of Dishwasher Use & Energy Consumption')\n","plt.subplot(1,3,3)\n","df.groupby('DWASHUSE')['KWH'].median().plot.bar()\n","plt.title('Median KWH for Frequency of Dishwasher Use')\n","plt.ylabel('KWH')"]},{"cell_type":"markdown","metadata":{"id":"Htr8ifRgQ-mu"},"source":["Based on the above plots, we can observe the following:\n","- From the barplot, we may observe that most of the households use their dishwashers '2 or 3 times a week' (depicted by factor/level value 13. Note that factor/level value 11 indicates use of dishwasher less than once a week and factor/level 30 indicates using dishwasher at least once each day)\n","- We can observe a clear pattern in the boxplot as well as median KWH plot above that more frequently household use dishwasher, more is the energy consumption. Thus, feature DWASHUSE has a direct relation with the target variable"]},{"cell_type":"markdown","metadata":{"id":"g4eaJiWEQ-mv"},"source":["**How does Household Demographic features relates with the target variable 'KWH'?**\n","\n","a) Relation Between Total Members in Household and Energy Consumption in KWH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XWipL08WQ-mv","executionInfo":{"status":"aborted","timestamp":1754631597081,"user_tz":-330,"elapsed":15855,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["plt.figure(figsize=(24,6))\n","plt.subplot(1,3,1)\n","sns.set_style('whitegrid')\n","ax0 = sns.countplot(x='NHSLDMEM', data = df)\n","ax0.set_title('Distribution of Total Members in Household')\n","plt.subplot(1,3,2)\n","ax1 = sns.boxplot(x='NHSLDMEM', y ='KWH', data = df)\n","ax1.set_title('Relationship b/w Total Members in Household & Energy Consumption')\n","plt.subplot(1,3,3)\n","df.groupby('NHSLDMEM')['KWH'].median().plot.bar()\n","plt.title('Median KWH for Total Members in Household')\n","plt.ylabel('KWH')"]},{"cell_type":"markdown","metadata":{"id":"otkijp_7Q-mv"},"source":["Based on the results of above bar plot, we can observe that very less number of households had more than 6 members in the household. The barplot and median KWH plot doesn't indicate a clear pattern of relationship between total members in household and energy consumption. However, if combine factors/levels 6-14 of feature NHSLDMEM together and create a new one 'more than 5 members, then we might observe a pattern indicating that as members in the household increases, the energy consumption increases as well. We will combine the levels in data preprocessing section"]},{"cell_type":"markdown","metadata":{"id":"meZ3ocO9Q-mv"},"source":["b) Relation Between Gross Household Income and Energy Consumption in KWH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HsAFk0NsQ-mv","executionInfo":{"status":"aborted","timestamp":1754631597118,"user_tz":-330,"elapsed":15886,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["plt.figure(figsize=(27,6))\n","plt.subplot(1,3,1)\n","sns.set_style('whitegrid')\n","ax0 = sns.countplot(x='MONEYPY', data = df)\n","ax0.set_title('Distribution of Gross Household Income')\n","plt.subplot(1,3,2)\n","ax1 = sns.boxplot(x='MONEYPY', y ='KWH', data = df)\n","ax1.set_title('Relationship b/w Gross Household Income & Energy Consumption')\n","plt.subplot(1,3,3)\n","df.groupby('MONEYPY')['KWH'].median().plot.bar()\n","plt.title('Median KWH for each category of Gross Household Income')\n","plt.ylabel('KWH')"]},{"cell_type":"markdown","metadata":{"id":"OjI3g1S7Q-mw"},"source":["Based on the results of above bar plot, we can observe that very less number of households had less than \\$10,000 as their gross household income (NOTE: factor/level 5 indicates gross household income of \\$10,000 to \\$14,999). The barplot and median KWH plot indicates higher energy consumption for households with gross income less than \\$2,500 (depicted by factor/level 1) as compared to households with gross income between \\$2,500 to \\$19,999 (i.e. factor/level 2 to 6). This could be because of free electricity subsidy programs by state and federal government. However, for households with gross income greater than \\$20,000, a clear pattern can be observed that as gross income of a household increase overall, the energy consumption increases as well"]},{"cell_type":"markdown","metadata":{"id":"JkJ7Gw9KQ-mw"},"source":["<a id=\"5\"></a>\n","### 5. Data Transformation and Preprocessing\n","\n","<a id=\"51\"></a>\n","#### a. Data Transformation\n","\n","<a id=\"511\"></a>\n","##### i. Combining/Merging Predictor Features\n","\n","Let's create a couple of new features by combining some of the existing features. These new features alongwith the existing ones will be later analyzed in feature selection section to determine whether keeping these features would help to understand our target variable better.\n","\n","Here's the list of new features that will be created in this section:\n","- **TV Equipment Features:** We will merge all the TV equipment related features (such as TV, VCR, DVD, game console, home theatre, cable box and set-top box) and create a new feature called ALLTVFTR. The features that we will combine are ['TVCOLOR', 'CABLESAT1', 'COMBODVR1','DVR1', 'DIGITSTB1', 'PLAYSTA1', 'COMBOVCRDVD1', 'VCR1', 'DVD1', 'TVAUDIOSYS1', 'OTHERSTB1', 'CABLESAT2', 'COMBODVR2', 'DVR2', 'DIGITSTB2', 'PLAYSTA2', 'COMBOVCRDVD2', 'VCR2', 'DVD2', 'TVAUDIOSYS2', 'OTHERSTB2', 'CABLESAT3', 'COMBODVR3', 'DVR3', 'DIGITSTB3', 'PLAYSTA3', 'COMBOVCRDVD3', 'VCR3', 'DVD3', 'TVAUDIOSYS3', 'OTHERSTB3']\n","\n","- **Office Equipment Features:** We will merge all the Office equipment related features (such as computer, monitor, printer, fax machine and cope machine) and create a new feature called ALLOFFFTR. The features that we will combine are ['NUMPC', 'PCPRINT', 'FAX', 'COPIER', 'MONITOR1', 'MONITOR2', 'MONITOR3']\n","\n","- **Telephone Features:** We will merge all the telephone equipment related features (such as cordless telephone and answering machine) and create a new feature called ALLTELFTR. The features that we will combine are ['NOCORD', 'ANSMACH']\n","\n","Let's just first start by replacing -2 value in all the above mentioned features to 0. Changing -2 to 0 in above features will ensure that we are not subtracting any value during the merging of these features. NOTE: None of the telephone features has -2 factor/level\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGscKBF5Q-mw","executionInfo":{"status":"aborted","timestamp":1754631597120,"user_tz":-330,"elapsed":15887,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["tv_features = ['TVCOLOR', 'CABLESAT1', 'COMBODVR1','DVR1', 'DIGITSTB1', 'PLAYSTA1', 'COMBOVCRDVD1', 'VCR1', 'DVD1', 'TVAUDIOSYS1', 'OTHERSTB1', 'CABLESAT2', 'COMBODVR2', 'DVR2',\n","'DIGITSTB2', 'PLAYSTA2', 'COMBOVCRDVD2', 'VCR2', 'DVD2', 'TVAUDIOSYS2', 'OTHERSTB2', 'CABLESAT3', 'COMBODVR3', 'DVR3', 'DIGITSTB3', 'PLAYSTA3', 'COMBOVCRDVD3', 'VCR3', 'DVD3',\n","'TVAUDIOSYS3', 'OTHERSTB3']\n","office_features = ['NUMPC', 'PCPRINT', 'FAX', 'COPIER', 'MONITOR1', 'MONITOR2', 'MONITOR3']\n","tel_features = ['NOCORD', 'ANSMACH']\n","\n","for i in tv_features:\n","    df[i] = df[i].apply(lambda x : x if x != -2 else 0)\n","df['ALLTVFTR'] = df[tv_features].sum(axis=1)\n","\n","for i in office_features:\n","    df[i] = df[i].apply(lambda x : x if x != -2 else 0)\n","df['ALLOFFFTR'] = df[office_features].sum(axis=1)\n","\n","df['ALLTELFTR'] = df[tel_features].sum(axis=1)"]},{"cell_type":"markdown","metadata":{"id":"rg7dZZMFQ-mw"},"source":["<a id=\"512\"></a>\n","##### ii. Combining/Merging Levels with Low Frequency in Discrete Predictor Features\n","\n","In the [Explore Additional Predictor Variables](#g-explore-additional-predictor-variables) section of EDA, we observed that some of the discrete numeric variables 'Total Rooms in Housing (TOTROOMS) and 'Total Members in Household' (NHSLDMEM) had only few data observations beyond a specific factor value. Hence, we will merge some of the levels for these variables due to the low frequency of values for these levels. In particular, for feature TOTROOMS, we will merge values 15 to 23 and create a new level 12 which would imply 'More than 14 Rooms'. For feature NHSLDMEM, we will merge values 6 to 14 to create a new level 6 which would mean 'More than 5 Members'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1QW7CwB2Q-mx","executionInfo":{"status":"aborted","timestamp":1754631597121,"user_tz":-330,"elapsed":15888,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df['TOTROOMS'] = df['TOTROOMS'].replace([12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23], [12]*12)\n","df['NHSLDMEM'] = df['NHSLDMEM'].replace([6, 7, 8, 9, 10, 11, 12, 13, 14], [6]*9)"]},{"cell_type":"markdown","metadata":{"id":"aipkgHBsQ-mx"},"source":["Let's verify whether the levels have been merged or not"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nPUwnRsfQ-mx","executionInfo":{"status":"aborted","timestamp":1754631597123,"user_tz":-330,"elapsed":15884,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["print(df['TOTROOMS'].describe(), df['TOTROOMS'].value_counts())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKGe2wScQ-mx","executionInfo":{"status":"aborted","timestamp":1754631597142,"user_tz":-330,"elapsed":15898,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["print(df['NHSLDMEM'].describe(), df['NHSLDMEM'].value_counts())"]},{"cell_type":"markdown","metadata":{"id":"brUTPu9nQ-my"},"source":["<a id=\"52\"></a>\n","#### b. Data Preprocessing\n","\n","<a id=\"521\"></a>\n","##### i. Removing Predictor Features with high 'Not Applicable' Values\n","\n","Previosuly, in the [Check for Missing Values](#d-check-for-missing-values) section of EDA we found 72 features with 'Not Applicable' values in more than 95% of the observations (i.e. less variability in the data values since >95% of observations are same value -2 i.e. NA). And these features were also checked for their correlation with our target variable and thus on the basis of high NA values and very weak correlation with target variable, we concluded that these 72 features have negligible amount of predictive power over the outcome variable. Hence, we will now drop these features from our dataset.\n","\n","In addition, in the [Summary Statistics](#c-summary-statistics) section of EDA, we also found two other features 'NOCRCASH' and 'NKRGALNC' to have more than 99% of the data values marked as -2 i.e. 'Not Applicable'. Hence, we will drop these features too since there is less variability in the data values and thus these features might not have any influence over the outcome variable"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"19qK8p1cQ-my","executionInfo":{"status":"aborted","timestamp":1754631597143,"user_tz":-330,"elapsed":15896,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.shape # let's print the shape before dropping above discussed 74 features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7-zk3jrSQ-my","executionInfo":{"status":"aborted","timestamp":1754631597145,"user_tz":-330,"elapsed":15894,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["highNA_features = list(na_col_names.values)\n","highNA_features.extend(['NOCRCASH', 'NKRGALNC']) # adding two more features to the list of features with high NA values\n","print(len(highNA_features))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D-2FCrt8Q-my","executionInfo":{"status":"aborted","timestamp":1754631597146,"user_tz":-330,"elapsed":15894,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.drop(highNA_features, axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zW9yNLQUQ-mz","executionInfo":{"status":"aborted","timestamp":1754631597148,"user_tz":-330,"elapsed":15892,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.shape # let's print the shape after dropping above discussed 74 features."]},{"cell_type":"markdown","metadata":{"id":"rCbr-WmoQ-mz"},"source":["<a id=\"522\"></a>\n","##### ii. Removing Imputation Flags\n","\n","All the features in our dataset starting with 'Z' are features representing imputation flags and we will now remove all of these features below."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ba0XNayfQ-mz","executionInfo":{"status":"aborted","timestamp":1754631597149,"user_tz":-330,"elapsed":15887,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["# Let's make a list of all the features starting with 'Z'\n","Z_features = [col for col in df if col.startswith('Z')]\n","print('We have {} imputation flag features'.format(len(Z_features)))\n","print('-----'*38)\n","print('These features are: \\n{}'.format(Z_features))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1kC-9hi6Q-mz","executionInfo":{"status":"aborted","timestamp":1754631597150,"user_tz":-330,"elapsed":15884,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.shape # let's print the shape before dropping above mentioned 359 features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jP5LLcT5Q-mz","executionInfo":{"status":"aborted","timestamp":1754631597167,"user_tz":-330,"elapsed":15900,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.drop(Z_features, axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XK8d6QXPQ-m0","executionInfo":{"status":"aborted","timestamp":1754631597169,"user_tz":-330,"elapsed":15899,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.shape # let's print the shape after dropping the imputation flag features."]},{"cell_type":"markdown","metadata":{"id":"PQnhq0qNQ-m0"},"source":["<a id=\"523\"></a>\n","##### iii. Removing Duplicate Features\n","\n","Previously, in the [Explore Target Variable and Extract Important Features](#e-explore-target-variable-and-extract-important-features) section of EDA, we found that some features present in the dataset to be duplicates such as KWH and BTUEL are both energy consumption features but in different units. Hence, we will drop all BTU features. In addition, we will drop price/cost features too since it is only consumption we are interested in. Moreover, cost is usually determined from the energy usage. Let's now make a list of all the BTU and DOL features that needs to be removed. Below is a list of all BTU, KWH and cost (columns with keyword DOL) related features. Some of these features were compiled by summing up one or more predictor features (as can be seen below:\n",")\n","- KWH = KWHSPH + KWHCOL + KWHWTH + KWHRFG + KWHOTH<br>\n","- BTUEL = BTUELSPH + BTUELCOL + BTUELWTH + BTUELRFG + BTUELOTH<br>\n","- DOLLAREL = DOLELSPH + DOLELCOL + DOLELWTH + DOLELRFG + DOLELOTH<br>\n","- CUFEETNG = CUFEETNGSPH + CUFEETNGWTH + CUFEETNGOTH<br>\n","- BTUNG = BTUNGSPH + BTUNGWTH + BTUNGOTH<br>\n","- DOLLARNG = DOLNGSPH + DOLNGWTH + DOLNGOTH<br>\n","- GALLONLP = GALLONLPSPH + GALLONLPWTH + GALLONLPOTH<br>\n","- BTULP = BTULPSPH + BTULPWTH + BTULPOTH<br>\n","- DOLLARLP = DOLLPSPH\t+ DOLLPWTH + DOLLPOTH<br>\n","- GALLONFO = GALLONFOSPH + GALLONFOWTH + GALLONFOOTH<br>\n","- BTUFO = BTUFOSPH + BTUFOWTH + BTUFOOTH<br>\n","- DOLLARFO = DOLFOSPH + DOLFOWTH + DOLFOOTH<br>\n","- GALLONKER = GALLONKERSPH + GALLONKERWTH + GALLONKEROTH<br>\n","- BTUKER = BTUKERSPH + BTUKERWTH + BTUKEROTH<br>\n","- DOLLARKER = DOLKERSPH + DOLKERWTH + DOLKEROTH<br>\n","- TOTALBTU = TOTALBTUSPH + TOTALBTUCOL + TOTALBTUWTH + TOTALBTURFG + TOTALBTUOTH<br>\n","- TOTALDOL = TOTALDOLSPH + TOTALDOLCOL + TOTALDOLWTH + TOTALDOLRFG + TOTALDOLOTH<br>\n","- TOTALBTUSPH = BTUELSPH + BTUNGSPH + BTULPSPH + BTUFOSPH + BTUKERSPH<br>\n","- TOTALBTUCOL = BTUELCOL<br>\n","- TOTALBTUWTH = BTUELWTH + BTUNGWTH + BTULPWTH +  + BTUFOWTH + BTUKERWTH<br>\n","- TOTALBTURFG = BTUELRFG<br>\n","- TOTALBTUOTH = BTUELOTH + BTUNGOTH + BTULPOTH + BTUFOOTH + BTUKEROTH<br>\n","- TOTALDOLSPH = DOLELSPH + DOLNGSPH + DOLLPSPH + DOLFOSPH + DOLKERSPH<br>\n","- TOTALDOLCOL = DOLELCOL<br>\n","- TOTALDOLWTH = DOLELWTH + DOLNGWTH + DOLLPWTH + DOLFOWTH + DOLKERWTH<br>\n","- TOTALDOLRFG = DOLELRFG<br>\n","- TOTALDOLOTH = DOLELOTH + DOLNGOTH + DOLLPOTH + DOLFOOTH + DOLKEROTH<br>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x2DWBJVJQ-m0","executionInfo":{"status":"aborted","timestamp":1754631597202,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["BTU_features = [col for col in df if col.startswith('BTU')]\n","# Let's also add TOTALBTU features to this list\n","BTU_features.extend([col for col in df if col.startswith('TOTALBTU')])\n","print('We have {} BTU features'.format(len(BTU_features)))\n","print('-----'*38)\n","print('These features are: \\n{}'.format(BTU_features))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hntk9rqiQ-m0","executionInfo":{"status":"aborted","timestamp":1754631597203,"user_tz":-330,"elapsed":3,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.shape # let's print the shape before dropping above mentioned 29 BTU features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fu8sfBzbQ-m1","executionInfo":{"status":"aborted","timestamp":1754631597205,"user_tz":-330,"elapsed":4,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.drop(BTU_features, axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PQj1Df_RQ-m1","executionInfo":{"status":"aborted","timestamp":1754631597206,"user_tz":-330,"elapsed":5,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.shape # let's print the shape after dropping the BTU features. Expected Shape:"]},{"cell_type":"markdown","metadata":{"id":"mNBMHGLQQ-m1"},"source":["Let's now make a list of all cost/price features i.e. features starting with keywords 'DOL' and 'TOTALDOL'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7a4BBBTzQ-m2","executionInfo":{"status":"aborted","timestamp":1754631597208,"user_tz":-330,"elapsed":7,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["DOL_features = [col for col in df if col.startswith('DOL')]\n","# Let's also add TOTALDOL features to this list\n","DOL_features.extend([col for col in df if col.startswith('TOTALDOL')])\n","print('We have {} DOL or cost features'.format(len(DOL_features)))\n","print('-----'*38)\n","print('These features are: \\n{}'.format(DOL_features))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZOEjDEpyQ-m2","executionInfo":{"status":"aborted","timestamp":1754631597209,"user_tz":-330,"elapsed":7,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.shape # let's print the shape before dropping above mentioned 28 DOL features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l2jjSKoMQ-m2","executionInfo":{"status":"aborted","timestamp":1754631597210,"user_tz":-330,"elapsed":15916,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.drop(DOL_features, axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z__XpmgeQ-m2","executionInfo":{"status":"aborted","timestamp":1754631597211,"user_tz":-330,"elapsed":15912,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.shape # let's print the shape after dropping the DOL features."]},{"cell_type":"markdown","metadata":{"id":"RcQHPHa6Q-m3"},"source":["Let's now drop the features related to Wood usage, LPG/Propane usage, Natural Gas usage, Fuel Oil usage and Kerosene usage. In addition, we will also drop the following KWH features 'KWHSPH', 'KWHCOL', 'KWHWTH', 'KWHRFG', 'KWHOTH' since our target variable is KWH"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j5tbFwD-Q-m3","executionInfo":{"status":"aborted","timestamp":1754631597212,"user_tz":-330,"elapsed":15909,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["othr_usage_features = ['KWHSPH', 'KWHCOL', 'KWHWTH', 'KWHRFG', 'KWHOTH', 'CUFEETNG', 'CUFEETNGSPH', 'CUFEETNGWTH', 'CUFEETNGOTH', 'GALLONLP', 'GALLONLPSPH',\n","'GALLONLPWTH', 'GALLONLPOTH', 'GALLONFO', 'GALLONFOSPH', 'GALLONFOWTH', 'GALLONFOOTH', 'GALLONKER', 'GALLONKERSPH', 'GALLONKERWTH', 'GALLONKEROTH', 'CORDSWD']\n","\n","print('We have {} other usage and additional KWH features'.format(len(othr_usage_features)))\n","print('-----'*38)\n","print('These features are: \\n{}'.format(othr_usage_features))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gG-zjr0GQ-m3","executionInfo":{"status":"aborted","timestamp":1754631597214,"user_tz":-330,"elapsed":15907,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.shape # let's print the shape before dropping above mentioned 22 other usage and additional KWH features."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"htcmYaLQQ-m4","executionInfo":{"status":"aborted","timestamp":1754631597244,"user_tz":-330,"elapsed":15937,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.drop(othr_usage_features, axis = 1, inplace = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xjGX7ZLeQ-m4","executionInfo":{"status":"aborted","timestamp":1754631597258,"user_tz":-330,"elapsed":15946,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.shape # let's print the shape after dropping the other usage and additional KWH features."]},{"cell_type":"markdown","metadata":{"id":"kHBXSTq4Q-m4"},"source":["Let's drop the feature DOEID as it is identifer rather than predictor. In addition, let's also drop NWEIGHT"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pdIdOiqgQ-m4","executionInfo":{"status":"aborted","timestamp":1754631597262,"user_tz":-330,"elapsed":15946,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df.drop(['DOEID', 'NWEIGHT'], axis = 1, inplace = True)\n","# Let's print the shape after droping these features.\n","df.shape"]},{"cell_type":"markdown","metadata":{"id":"Mplh1lAJQ-m5"},"source":["<a id=\"524\"></a>\n","##### iv. Removing Outliers\n","\n","Previously, in the [Explore Target Variable and Extract Important Features](#e-explore-target-variable-and-extract-important-features) section of EDA, we found that almost all the observations had KWH value under 80,000. Let's check how many observations in the dataset have KWH greater than 80,000 and accordingly handle these outlier values (if any)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vBcLbF0mQ-m5","executionInfo":{"status":"aborted","timestamp":1754631597263,"user_tz":-330,"elapsed":15943,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["# Let's check how many records have KWH > 80,000\n","print(f\"Number of records that have more than 80000 KWH: {df[df.KWH > 80000].shape}\")"]},{"cell_type":"markdown","metadata":{"id":"r4syBuJ4Q-m5"},"source":["As we can see above, only one record has been found to have more than 80000 KWH value and in the previous section we saw that this oulier value of KWH was indeed very high to be true. Hence, we will now drop this row from our dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PSBXPmSYQ-m5","executionInfo":{"status":"aborted","timestamp":1754631597264,"user_tz":-330,"elapsed":15939,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df2 = df[df.KWH <= 80000]\n","df2.shape"]},{"cell_type":"markdown","metadata":{"id":"QwnBMT82Q-m6"},"source":["Let's also verify visually from boxplot whether this extreme outlier value has been excluded or not"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2MkSfTqQ-m6","executionInfo":{"status":"aborted","timestamp":1754631597265,"user_tz":-330,"elapsed":15936,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["sns.set(style=\"darkgrid\")\n","fig, ax = plt.subplots(figsize = (10,6))\n","ax = sns.boxplot(x=df2['KWH']).set(title = 'KWH Boxplot')"]},{"cell_type":"markdown","metadata":{"id":"3Uobf6NEQ-m6"},"source":["<a id=\"6\"></a>\n","### 6. Feature Engineering\n","\n","<a id=\"61\"></a>\n","#### a. Exploratory Feature Reduction\n","\n","Let's use Principle Component Analysis (PCA), a feature reduction technique to see how many features can be used for data modeling. In other words, PCA is a technique to obtain important features from a large set of features which explains the most of the variability in the data. Let's start implementing PCA by first removing response variable from the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vWFfroP_Q-m6","executionInfo":{"status":"aborted","timestamp":1754631597266,"user_tz":-330,"elapsed":15932,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df_tmp = df2.drop(['KWH', 'KWHlog'], axis = 1)\n","df_tmp.shape"]},{"cell_type":"markdown","metadata":{"id":"1E1wzDR6Q-m7"},"source":["Let's use One-hot Encoding, a feature encoding strategy, first to convert our categorical features into a numerical feature"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iW4Cg6kGQ-m7","executionInfo":{"status":"aborted","timestamp":1754631597267,"user_tz":-330,"elapsed":15932,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["df_tmp = pd.get_dummies(df_tmp, columns= cat_features_remaining, drop_first = True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nJtlfu_YQ-m7","executionInfo":{"status":"aborted","timestamp":1754631597309,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["from sklearn.decomposition import PCA\n","from sklearn.preprocessing import scale\n","\n","# Convert to numpy and scale\n","X = scale(df_tmp.values)\n","\n","# Use a valid number of components (â‰¤ min(n_samples, n_features))\n","pca = PCA(n_components=428)\n","pca.fit(X)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BkexYymzQ-m7","executionInfo":{"status":"aborted","timestamp":1754631597312,"user_tz":-330,"elapsed":8,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["var= pca.explained_variance_ratio_"]},{"cell_type":"markdown","metadata":{"id":"xDVHo6lmQ-m7"},"source":["Let's build a scree plot i.e. a line plot that shows the eigenvalues for each individual principal component. Scree plot helps us to access components or factors which explains the most of variability in the data. It represents values in descending order."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"keVXyP_iQ-m8","executionInfo":{"status":"aborted","timestamp":1754631597313,"user_tz":-330,"elapsed":8,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["plt.grid(which='major', linestyle='--', linewidth='0.5', color='grey')\n","plt.title('Scree Plot')\n","plt.xlabel('Principal Component')\n","plt.ylabel('Proportion of Variance Explained')\n","plt.plot(var)"]},{"cell_type":"markdown","metadata":{"id":"fwW1DC2wQ-m8"},"source":["The above scree plot indicates that approx. 20 principal components were able to capture most of the information."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"erl0jJ39Q-m8","executionInfo":{"status":"aborted","timestamp":1754631597314,"user_tz":-330,"elapsed":8,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["#Cumulative Variance explains\n","var1=np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n","print(var1)"]},{"cell_type":"markdown","metadata":{"id":"jrD7QbFtQ-m8"},"source":["The above cumulative variance can be read as follows:\n","It shows that first principal component explains 10.1% variance. The first and second component cumulatively explains 15.05% variance or we can say that second principal component alone explains 4.95% variance. First, second and third component cumulatively explains 19.05% variance and so on. Let's now plot cumulative variance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O68izlAyQ-m9","executionInfo":{"status":"aborted","timestamp":1754631597316,"user_tz":-330,"elapsed":10,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["# cumulative scree plot\n","plt.grid(which='major', linestyle='-', linewidth='0.5', color='red')\n","plt.title('Cumulative Variance Plot')\n","plt.xlabel('Number of Features')\n","plt.ylabel('Cumulative Proportion of Variance Explained')\n","plt.plot(var1)"]},{"cell_type":"markdown","metadata":{"id":"Jbar09wYQ-m9"},"source":["The above plot shows that ~ 200 features result in variance close to ~ 95%. The PCA analysis gives us a ballpark estimate of the number of features that explains majority of the variation in the dataset and hence can be used for data modeling. However, let's use Gradient Boosting Machine (GBM) in the later [Feature Selection](#) section to automatically calculate feature importance\n","\n","<a id=\"62\"></a>\n","#### b. Feature Selection\n","\n","I am going to use a pre built `FeatureSelector` class made available by Will Koehrsen on their [GitHub](https://github.com/WillKoehrsen/feature-selector/blob/master/feature_selector/feature_selector.py). This `FeatureSelector` includes the following most common feature selection methods:\n","\n","- Features with a high percentage of missing values\n","- Collinear (highly correlated) features\n","- Features with zero importance in a tree-based model\n","- Features with low importance\n","- Features with a single unique value\n","\n","Earlier, during the EDA phase of the project, we found that there were indeed no missing values in the RECS 2009 dataset. Hence, the `FeatureSelector` class that we are going to utilize for Feature Selection phase of the project, we are only going to use the last four common feature selection methods. Let's now start by finding features with single unique value\n","\n","<a id=\"621\"></a>\n","##### i. Find Features with Single Unique Value\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hvp3mS8hQ-m9","executionInfo":{"status":"aborted","timestamp":1754631597317,"user_tz":-330,"elapsed":10,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["labels = df2['KWH']\n","data_fs = df2.drop(columns = ['KWH', 'KWHlog'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oU32jr_WQ-m9","executionInfo":{"status":"aborted","timestamp":1754631597318,"user_tz":-330,"elapsed":11,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["fs = FeatureSelector(data = data_fs, labels = labels) # Let's inititate the class FeatureSelector()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EVtYkBuNQ-m-","executionInfo":{"status":"aborted","timestamp":1754631597319,"user_tz":-330,"elapsed":11,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["fs.identify_single_unique()\n","single_unique = fs.ops['single_unique']\n","single_unique"]},{"cell_type":"markdown","metadata":{"id":"rqy0cSGNQ-m-"},"source":["As we can see above, we just have one feature 'USEEL' in the entire set of 441 features with single unique value. Let's now check the collinear features in our dataset. Collinear features can be defined as the features that are highly correlated with one another. In practice, we exclude the features which are highly correlated as including a pair of such features can lead to decrease in model performance on test set.\n","\n","<a id=\"622\"></a>\n","##### ii. Find Collinear Features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tEmusrwNQ-m-","executionInfo":{"status":"aborted","timestamp":1754631597366,"user_tz":-330,"elapsed":57,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["fs.identify_collinear(correlation_threshold=0.98)\n","correlated_features = fs.ops['collinear']\n","print('----------------------------------------------------------------------------------------')\n","print('These features are: \\n{}'.format(correlated_features))"]},{"cell_type":"markdown","metadata":{"id":"25xvwnrGQ-m-"},"source":["Let's now plot all the correlations which were found to be above threshold using correlation heatmap of the correlation values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-K1r8pOpQ-m_","executionInfo":{"status":"aborted","timestamp":1754631597368,"user_tz":-330,"elapsed":48,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["features = list(set(fs.record_collinear['corr_feature']).union(set(fs.record_collinear['drop_feature'])))\n","valid_features = [f for f in features if f in df2.columns]\n","corr_mat_plt = df2[valid_features].corr()"]},{"cell_type":"markdown","metadata":{"id":"WEIuDp8jQ-m_"},"source":["Let's also check which pair of features were found to be correlated with each other and their respective correlation values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HkRvxpfCQ-m_","executionInfo":{"status":"aborted","timestamp":1754631597368,"user_tz":-330,"elapsed":5,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["fs.record_collinear"]},{"cell_type":"markdown","metadata":{"id":"sgu1yW6KQ-m_"},"source":["<a id=\"623\"></a>\n","##### iii. Find Features with Zero Importance using GBM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gvBJWgCqQ-nA","executionInfo":{"status":"aborted","timestamp":1754631597369,"user_tz":-330,"elapsed":5,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["fs.identify_zero_importance(task = 'regression', eval_metric = 'l2', n_iterations = 10, early_stopping = False)\n","print('----------------------------------------------------------------------------------------')\n","print('These features are: \\n{}'.format(fs.ops['zero_importance']))"]},{"cell_type":"markdown","metadata":{"id":"Ru3IyfmaQ-nA"},"source":["The above method of feature selection has been designed for machine learning problem. Using Gradient Boosting Machine learning model from the [LightGBM Library](https://lightgbm.readthedocs.io/en/v3.3.2/), we tried to find the features from our dataset which have zero importance. These features were averaged over 10 training runs in order to reduce variance. As we can observe from the above results, 28 features were found to have zero importance and we can remove these features later on without affecting model performance.\n","\n","Let's now check how many features have a cumulative importance of 90% and also see the top 20 features in order of their importance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wv2-iUOpQ-nA","executionInfo":{"status":"aborted","timestamp":1754631597370,"user_tz":-330,"elapsed":5,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["fs.plot_feature_importances(threshold = 0.9, plot_n = 20)"]},{"cell_type":"markdown","metadata":{"id":"OP4-exO0Q-nA"},"source":["As can be seen from the first plot, among the top 20 features (plotted in terms of normalized importance), the top five of them are YEARMADE, HHAGE, TOTCSQFT, CDD80 and CDD30YR. The plot at the bottom shows cumulative importance ploteed on y-axis and number of features on x-axis. The vertical dotted line is drawn at the threshold value of cumulative importance we chose above i.e. 90%. We can notice that 225 features are required for 90% of the cumulative importance."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pw0PctlpQ-nB","executionInfo":{"status":"aborted","timestamp":1754631597371,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["top_features = list(fs.feature_importances.loc[:224, 'feature'])\n","print(top_features)"]},{"cell_type":"markdown","metadata":{"id":"lQaOznCEQ-nB"},"source":["<a id=\"624\"></a>\n","##### iv. Find Features with Low Importance\n","\n","Let's now find the lowest importance features that do not contribute to a specified total importance. Recall that when finding important features in the section above, we used a cumulative importance threshold of 90%. Let's use the same threshold value to find the least important features that are not required to achieve 90% of total importance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WugmFAC8Q-nB","executionInfo":{"status":"aborted","timestamp":1754631597374,"user_tz":-330,"elapsed":8,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["fs.identify_low_importance(cumulative_importance = 0.90)"]},{"cell_type":"markdown","metadata":{"id":"bYX2ChNdQ-nC"},"source":["As you can notice, based on the cumulative importance threshold value of 90%, the gradient boosting machine considers 217 features to be not relevant for model learning purpose.\n","<a id=\"625\"></a>\n","##### v. Removing Features\n","\n","Using above four methods in [Feature Selection](#b-feature-selection), we identifed which features to drop. Now' let's drop all these features"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tbv-dBicQ-nC","executionInfo":{"status":"aborted","timestamp":1754631597375,"user_tz":-330,"elapsed":15985,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["data_fs_removed = fs.remove(methods = 'all')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FrXVbA5zQ-nC","executionInfo":{"status":"aborted","timestamp":1754631597377,"user_tz":-330,"elapsed":15982,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["print(list(data_fs_removed.columns))\n","print(len(data_fs_removed.columns))"]},{"cell_type":"markdown","metadata":{"id":"PvMjtwg3Q-nD"},"source":["Let's remove the one hot encoding features from the above `data_fs_removed` dataframe i.e. we will remove all the columns which were categorical in nature and were converted to one hot encodings during the process of feature selection. These columns are METROMICRO_METRO, UR_R and IECC_Climate_Pub_3A. The reason we are doing this is because we ran feature selection on the entire training set without separating the test set. In reality, while training the model, we would separate the dataset into two subsets, training and test. The training set will be further divided into two splits during cross validation, training and validation fold, and we don't have any information beforehand whether we would have categories in the test data that were not in the training data. Usually, if such case arise, an error will occur. In addition, another way to think about this is considering the deployment stage of the model. There is a chance that the data distributions in future might change and we might get new categories in our categorical data features and the model prediction will result in error if there are new categories in the dataset. Hence, we would use the `ColumnTransformer` inside the `Pipelines` in the later section of model development to handle the categorical features.\n","\n","However, it's worth mentioning here that of all the categories of the categorical features, only three, METROMICRO_METRO, UR_R and IECC_Climate_Pub_3A, were found to be important features by the `FeatureSelector` class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1wmkrBMYQ-nD","executionInfo":{"status":"aborted","timestamp":1754631597380,"user_tz":-330,"elapsed":15981,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["data_fs_removed.shape"]},{"cell_type":"markdown","metadata":{"id":"nq2bio71Q-nD"},"source":["Now, let's add original categorical feature names to the list of columns of data_fs_removed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"24TjL9VqQ-nD","executionInfo":{"status":"aborted","timestamp":1754631597381,"user_tz":-330,"elapsed":15978,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["print('Original Number of Features', data_fs.shape[1])\n","print('Final Number of Features: ', data_fs_removed.shape[1])"]},{"cell_type":"markdown","metadata":{"id":"GikGuGsLQ-nE"},"source":["Let's append our target variable column to final_columns list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRaPhrW3Q-nE","executionInfo":{"status":"aborted","timestamp":1754631597447,"user_tz":-330,"elapsed":13,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["final_columns = list(data_fs_removed.columns)\n","final_columns.append(\"KWH\")"]},{"cell_type":"markdown","metadata":{"id":"KrPqAwTnQ-nE"},"source":["<a id=\"7\"></a>\n","### 7. Model Development & Comparison\n","<a id=\"71\"></a>\n","#### a. Building Baseline Models with default params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cYo2_ug-Q-nE","executionInfo":{"status":"aborted","timestamp":1754631597449,"user_tz":-330,"elapsed":14,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["final_data = df2[final_columns]\n","final_data.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbSxj9oqQ-nE","executionInfo":{"status":"aborted","timestamp":1754631597450,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["# Separate features and labels\n","y = final_data['KWH']\n","X = final_data.drop(columns =['KWH'])"]},{"cell_type":"markdown","metadata":{"id":"EJLpy7tdQ-nF"},"source":["We could train a model using all the data we have; however it is a common practice in supervised machine learning to split into two subsets; a larger set with which to train the model, and a smaller holdout data set (also called test set) to provide an unbiased evaluation of a final model fit on the training data set.\n","\n","Next we do the train-test split and hold out the test set until we decide our final model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I1xKgV0HQ-nF","executionInfo":{"status":"aborted","timestamp":1754631597452,"user_tz":-330,"elapsed":6,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=0)\n","\n","print ('Training Set: %d rows\\nTest Set: %d rows' % (X_train.shape[0], X_test.shape[0]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kqTVNxuyQ-nF","executionInfo":{"status":"aborted","timestamp":1754631597453,"user_tz":-330,"elapsed":16037,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["print(f'X_train shape is:{X_train.shape} \\nX_test shape is:{X_test.shape}\\ny_train shape is:{y_train.shape}\\ny_test shape is:{y_test.shape}')"]},{"cell_type":"markdown","metadata":{"id":"fskbivGLQ-nG"},"source":["Next, we normalize the numeric features using `StandardScaler()` to transform our feature data such that its distribution will have a mean value 0 and standard deviation of 1. Normalizing is an important step in machine learning as it brings all the features on the same scale and thus prevents features with large values from producing coefficients that disproportionately affect the predictions. In addition to scaling transformation, we also need to apply one hot encoding to our categorical features to convert categories into numbers since scikit-learn only accepts numeric data as input. We will make use of `ColumnTransformer` by defining the separate preprocessing pipelines, each for numeric and categorical features.\n","\n","We will then wrap the column transformer in another pipeline containing our regressor using the `Pipeline` utility available in sklearn and finally use this pipeline inside `cross_validate`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Y8j9yJhQ-nG","executionInfo":{"status":"aborted","timestamp":1754631597453,"user_tz":-330,"elapsed":16036,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["numeric_features = X_train.select_dtypes('number').columns\n","categorical_features = X_train.select_dtypes('object').columns"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2n2xo_8pQ-nG","executionInfo":{"status":"aborted","timestamp":1754631597455,"user_tz":-330,"elapsed":16038,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["numeric_transformer = Pipeline(\n","    steps = [(\"scaler\", StandardScaler())]\n",")\n","\n","categorical_transformer = Pipeline(\n","    steps = [(\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]\n",")\n","\n","col_transformer = ColumnTransformer(\n","    transformers = [\n","        (\"numeric\", numeric_transformer, numeric_features),\n","        (\"categorical\", categorical_transformer, categorical_features)\n","]\n",")"]},{"cell_type":"markdown","metadata":{"id":"j8tcy38MQ-nH"},"source":["#### Train Baseline Models with Default Params\n","\n","Now we start training baseline model using default hyperparameters. We will use cross validation process in model training. If we were to fit the model with the training set while evaluated with the test set, we obtained only a single sample point of evaluation with one test set. How can we be sure it is an accurate evaluation, rather than a value too low or too high by chance? If we have two models, and found that one model is better than another based on the evaluation, how can we know this is also not by chance?\n","\n","Hence, we make use of cross validation to evaluate each model multiple times with different dataset and take the average score for our decision to choose the final model candidate for evaluation on holdout dataset or test dataset. __Cross validation__ uses k-fold to resample the same dataset multiple times and pretend they are different. With cross validation, as we are evaluating the model, or hyperparameter, the model has to be trained from scratch, each time, without reusing the training result from previous attempts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KRtuOcxQ-nH","executionInfo":{"status":"aborted","timestamp":1754631597460,"user_tz":-330,"elapsed":4,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["pipeline = []\n","pipeline.append((\"Linear Regression\", Pipeline([(\"preprocessor\", col_transformer), (\"LR\", LinearRegression())])))\n","pipeline.append((\"Lasso\", Pipeline([(\"preprocessor\", col_transformer), ('Lasso', Lasso())])))\n","pipeline.append((\"Ridge\", Pipeline([(\"preprocessor\", col_transformer), ('Ridge', Ridge())])))\n","pipeline.append((\"ElasticNet\", Pipeline([(\"preprocessor\", col_transformer), ('eNet', ElasticNet())])))\n","pipeline.append((\"RForest\", Pipeline([(\"preprocessor\", col_transformer), ('RF', RandomForestRegressor())])))\n","pipeline.append((\"Gradient Boosting\", Pipeline([(\"preprocessor\", col_transformer), ('GBM', GradientBoostingRegressor())])))\n","pipeline.append((\"XG Boost\", Pipeline([(\"preprocessor\", col_transformer), ('xgb', xgb.XGBRegressor(objective = \"reg:squarederror\"))])))"]},{"cell_type":"markdown","metadata":{"id":"cRlPKlWOQ-nH"},"source":["Let's define the scoring criteria by selecting:\n","- __Root Mean Square Error (RMSE):__ The square root of the mean of the squared difference between predicted and actual values. This yields an absolute metric in the same unit as the label (in this case, Electricity Consumption in KWH). The smaller the value, the better the model is (i.e. in a simplistic sense, it represents the average electricity consumption by which the predictions are wrong!)\n","\n","- __Coefficient of Determination (usually known as R-squared or R2):__ Higher the value of this metric, the better the fit of the model is. This metric represents how much of the variance between predicted and actual label values the model is able to explain. The R-squared metric might not be considered to be a good metric in our case because R-square value increases artificially as the number of features increases. Hence, we will set RMSE to be the main scoring criteria later in the hyperparameter tuning section\n","\n","You may use other metrics for evaluation regression models. Refer this [Scikit-Learn documentation](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N2lZdw_WQ-nI","executionInfo":{"status":"aborted","timestamp":1754631597479,"user_tz":-330,"elapsed":22,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["RMSE = []\n","R2 = []\n","names = []\n","scoring = {'rmse': 'neg_root_mean_squared_error',\n","           'r2': 'r2'\n","           }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hMrvub0nQ-nI","executionInfo":{"status":"aborted","timestamp":1754631597481,"user_tz":-330,"elapsed":24,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["for name, model in pipeline:\n","    kfold = KFold(n_splits = 5, random_state = 1, shuffle = True)\n","    cv_results = cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring, n_jobs = 1)\n","    RMSE.append(cv_results['test_rmse']*-1)\n","    R2.append(cv_results['test_r2'])\n","    names.append(name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k84Qk6FxQ-nI","executionInfo":{"status":"aborted","timestamp":1754631597482,"user_tz":-330,"elapsed":24,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["avg_RMSE = [sum(i)/len(i) for i in RMSE]\n","avg_R2 = [sum(j)/len(j) for j in R2]\n","model_baseline_metrics = pd.DataFrame({\n","                                            'Model': names,\n","                                            'Baseline_AvgRMSE': avg_RMSE,\n","                                            'Baseline_AvgR2': avg_R2\n","                                        })\n","model_baseline_metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4kYbIxrWQ-nJ","executionInfo":{"status":"aborted","timestamp":1754631597483,"user_tz":-330,"elapsed":24,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["plt.figure(figsize=(21,9))\n","\n","ax1 = plt.subplot(1,2,1)\n","sns.boxplot(data = RMSE)\n","ax1.set_xticklabels(names)\n","ax1.set_ylabel(\"Root Mean Square Error (RMSE)\")\n","ax1.set_title(\"Baseline Model RMSE comparsion across 7 models\")\n","\n","ax2 = plt.subplot(1,2,2)\n","sns.boxplot(data = R2)\n","ax2.set_xticklabels(names)\n","ax2.set_ylabel(\"r2 score\")\n","ax2.set_title(\"Baseline Model r2 comparsion across 7 models\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0C-JIrzxQ-nJ"},"source":["Above, we evaluated each of the models multiple times with different dataset using cross validation process. From the dataframe and box plots above, we can see that the baseline models more or less perform similar. Let's re-run cross validation after tuning the model hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"0T0oczqKQ-nJ"},"source":["<a id=\"72\"></a>\n","#### b. Hyperparameter Tuning & Model Comparison"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PwPXem_FQ-nK","executionInfo":{"status":"aborted","timestamp":1754631597484,"user_tz":-330,"elapsed":25,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["pipeline_tuned_models = []\n","pipeline_tuned_models.append((\"Lasso\", Pipeline([(\"preprocessor\", col_transformer), ('Lasso', Lasso(random_state = 123))])))\n","pipeline_tuned_models.append((\"Ridge\", Pipeline([(\"preprocessor\", col_transformer), ('Ridge', Ridge(random_state = 123))])))\n","pipeline_tuned_models.append((\"ElasticNet\", Pipeline([(\"preprocessor\", col_transformer), ('eNet', ElasticNet(random_state = 123))])))\n","pipeline_tuned_models.append((\"RForest\", Pipeline([(\"preprocessor\", col_transformer), ('RF', RandomForestRegressor(random_state = 123))])))\n","pipeline_tuned_models.append((\"Gradient Boosting\", Pipeline([(\"preprocessor\", col_transformer), ('GBM', GradientBoostingRegressor(random_state = 123))])))\n","pipeline_tuned_models.append((\"XG Boost\", Pipeline([(\"preprocessor\", col_transformer), ('xgb', xgb.XGBRegressor(objective = \"reg:squarederror\", random_state = 123))])))"]},{"cell_type":"markdown","metadata":{"id":"SPzHUYYNQ-nK"},"source":["Next, we define hyperparameters we want to tune for each of the models. Note: We only tuned few hyperparameters; however if you are not constrained by computational resources you may try more hyperparameters or more settings of hyperparameters to tune"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DraMv81_Q-nK","executionInfo":{"status":"aborted","timestamp":1754631597485,"user_tz":-330,"elapsed":16055,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["# Initiaze the hyperparameters for each dictionary\n","param1 = {}\n","param1['Lasso__alpha'] = np.linspace(0, 0.2, 21)\n","\n","param2 = {}\n","param2['Ridge__alpha'] = np.linspace(0, 0.2, 21)\n","\n","param3 = {}\n","param3['eNet__max_iter'] = [1, 5, 10]\n","param3['eNet__alpha'] = np.linspace(0, 0.2, 21)\n","param3['eNet__l1_ratio'] = np.arange(0.0, 1.0, 0.1)\n","\n","param4 = {}\n","param4['RF__n_estimators'] = [50, 100, 150]\n","max_depth = [5, 10, 15]       ##[int(x) for x in np.linspace(5, 15, num = 3)]\n","max_depth.append(None)\n","param4['RF__max_depth'] = max_depth\n","param4['RF__min_samples_split'] = [2, 5, 10]\n","# param4['min_samples_leaf'] = [5, 10, 15]  ## if you have enough computing respurces, you could try to tune more hyperparameters by uncommenting these lines\n","# param4['bootstrap'] = [True, False]\n","param4['RF__max_features'] = [1, 'sqrt']\n","# param4['max_leaf_nodes'] = [5, 10, 15]\n","\n","param5 = {}\n","param5['GBM__learning_rate'] = [0.01, 0.05, 0.1, 1.0]\n","param5['GBM__n_estimators'] = [50, 100, 150]\n","\n","param6 = {}\n","param6['xgb__learning_rate'] = [0.01, 0.05, 0.1]\n","param6['xgb__n_estimators'] = [100, 300, 600, 900]\n","param6['xgb__max_depth'] = [8]\n","param6['xgb__subsample'] = [0.7]\n","param6['xgb__colsample_bytree'] = [0.9]\n","param6['xgb__verbosity'] = [0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5QJ6FOFWQ-nL","executionInfo":{"status":"aborted","timestamp":1754631597486,"user_tz":-330,"elapsed":16051,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["params = [param1, param2, param3, param4, param5, param6]\n","names_tunedmodels = []\n","best_params = []\n","best_score = []\n","best_estimator = []\n","avg_RMSE = []\n","RMSE_tunedmodels = []\n","avg_R2 = []\n","R2_tunedmodels = []\n","for param, (name_t, pipe) in zip(params, pipeline_tuned_models):\n","    gridsearch = GridSearchCV(pipe, param_grid=param, scoring=scoring, cv=5, n_jobs=-1, refit='rmse', verbose=1)\n","    gridsearch.fit(X_train, y_train)\n","    best_params.append(gridsearch.best_params_)\n","    best_score.append(gridsearch.best_score_)\n","    best_estimator.append(gridsearch.best_estimator_)\n","    avg_RMSE.append(np.mean(gridsearch.cv_results_['mean_test_rmse']*-1))\n","    avg_R2.append(np.mean(gridsearch.cv_results_['mean_test_r2']))\n","    RMSE_tunedmodels.append(gridsearch.cv_results_['mean_test_rmse']*-1)\n","    R2_tunedmodels.append(gridsearch.cv_results_['mean_test_r2'])\n","    names_tunedmodels.append(name_t)\n","list_of_models = list(zip(names_tunedmodels, best_params, best_estimator, best_score, avg_RMSE, avg_R2))\n","df_models_tuned = pd.DataFrame(list_of_models, columns = ['Model', 'best_params', 'best_estimator', 'best_score_rmse', 'AvgRMSE', 'AvgR2'])\n","df_models_tuned"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_u8sGqDVQ-nL","executionInfo":{"status":"aborted","timestamp":1754631597488,"user_tz":-330,"elapsed":16048,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["plt.figure(figsize=(21,9))\n","\n","ax1 = plt.subplot(1,2,1)\n","sns.boxplot(data = RMSE_tunedmodels)\n","ax1.set_xticklabels(names_tunedmodels)\n","ax1.set_ylabel(\"Root Mean Square Error (RMSE)\")\n","ax1.set_title(\"Tuned Model RMSE comparsion across 6 models\")\n","\n","ax2 = plt.subplot(1,2,2)\n","sns.boxplot(data = R2_tunedmodels)\n","ax2.set_xticklabels(names_tunedmodels)\n","ax2.set_ylabel(\"r2 score\")\n","ax2.set_title(\"Tuned Model r2 comparsion across 6 models\")\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_Tz7XXP2Q-nL"},"source":["From the above rsult of cross validation using `GridSearchCV` (refer dataframe and box plots above), we can clearly see that the tree based models (i.e. Gradient Boosting Regressor and Xtreme Gradient Boosting XGB) have better RMSE compared to linear models. However; XGB model has the lowest RMSE in comparison to Gradient Boosting Regressor. Hence, we may conclude that XGB is better than GBM. For now, since XGB model is better than other models, we will retrain the model again using the set of best parameters of XGB that `GridSearchCV` found.\n","\n","The reason for retraining the model is that during the cross validation we do not have a lot of data, and the smaller dataset we used previously, had a part of it held out for validation. We believe that combining the training and validation dataset can produce a better model. Hence, we retrain the model of the entire training dataset this time and evaluate the model on our holdout dataset i.e. test dataset. Because this is unseen data, it can help us evaluate the generalization, or out-of-sample, error. This should simulate what the model will do when we deploy it. We do not expect this evaluation score to be very different from that we obtained from cross validation in the previous step, if we did the model training correctly. This can serve as a confirmation for our model selection.\n","\n","__NOTE:__ The dataset for evaluation on test dataset, and the one we used in cross validation, are different because we do not want data leakage. If they were the same, we would see the same score as we have already seen from the cross validation. After retraining the model, we will use test dataset. Since we used refit = 'rmse' inside `GridSearchCV`, the best estimator has already been refitted using the best found parameters on the whole training dataset. As a next step, we just need to call predict on `gridsearch.best_estimator_` using X_test dataset i.e. holdout because the 'best_estimator' is a pipeline containing both the `ColumnTransformer` and the trained model that had the best score."]},{"cell_type":"markdown","metadata":{"id":"TDfZYhYZQ-nM"},"source":["<a id=\"73\"></a>\n","#### c. Model Evaluation on Unseen Data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6o3LsFpEQ-nM","executionInfo":{"status":"aborted","timestamp":1754631597489,"user_tz":-330,"elapsed":16044,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["# Get Predictions\n","predictions = gridsearch.best_estimator_.predict(X_test)\n","\n","# Display Metrics\n","mse = mean_squared_error(y_test, predictions)\n","print(\"MSE:\", mse)\n","rmse = np.sqrt(mse)\n","print(\"RMSE:\", rmse)\n","r2 = r2_score(y_test, predictions)\n","print(\"R2:\", r2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S5y_Il30Q-nM","executionInfo":{"status":"aborted","timestamp":1754631597490,"user_tz":-330,"elapsed":16041,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["# Plot predicted vs actual\n","plt.scatter(y_test, predictions)\n","plt.xlabel('Actual Labels')\n","plt.ylabel('Predicted Labels')\n","plt.title('Energy Consumption Predictions')\n","# overlay the regression line\n","z = np.polyfit(y_test, predictions, 1)\n","p = np.poly1d(z)\n","plt.plot(y_test,p(y_test), color='magenta')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4D_0CGXBQ-nM","executionInfo":{"status":"aborted","timestamp":1754631597491,"user_tz":-330,"elapsed":16037,"user":{"displayName":"Dumbala Rushika","userId":"00661390307235985587"}}},"outputs":[],"source":["import os\n","import joblib\n","\n","# Ensure 'models/' folder exists\n","os.makedirs(\"models\", exist_ok=True)\n","\n","# Save the model\n","filename = 'models/xgb.pkl'\n","joblib.dump(gridsearch.best_estimator_, filename)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"},"vscode":{"interpreter":{"hash":"20262a1f36e699f4f2ad8d6f42803ba3f6c5cee5a6094f1ac859747a09ae26e5"}},"colab":{"provenance":[{"file_id":"1irU9iroB8SBOmBWd_rqRUYZBetTgcBRb","timestamp":1754116825987},{"file_id":"1ZO7Wkj3Irw8wvQBor23PA3YoZvDxEsJt","timestamp":1754063210215}]}},"nbformat":4,"nbformat_minor":0}